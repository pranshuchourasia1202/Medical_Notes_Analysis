# Medical Notes Analysis
Attempt to get strongest word embeddings in Medical Domain.

Presentation Slides : https://docs.google.com/presentation/d/1HeWvbQ3S6kN0BCu4cwz6a_vTuXcJ1opvvdlbOSJKueU/edit?usp=sharing
Google Drive Project : https://drive.google.com/drive/folders/1K9Z06YhgH_5F_bwA8-iI0sZOc45J-m_t?usp=sharing


# This file give brief introduction about functions implemented in "utils.py" - main utility file.

1. def get_components(term_matching) :
    The given code defines a function named "get_components" that takes a parameter "term_matching" as input. The function creates an undirected graph using the "NetworkX" library and adds edges to the graph from the input dataframe "term_matching" that contains two columns "Term1" and "Term2" representing the connected terms. Then it finds the connected components of the graph using the "connected_components" method from the "NetworkX" library, and returns the list of connected components.


2. def get_plain_embedding_similarity(mod, model, tokenizer, term_matching, word_pairs):
    The given code defines a function named "get_plain_embedding_similarity" that takes five input parameters: "mod", "model", "tokenizer", "term_matching", and "word_pairs". This function is used to calculate the cosine similarity between the plain embeddings of a list of word pairs generated by a transformer model.
    The function initializes an empty list called "embeddings" to store the embeddings generated for each word pair.
    The model is moved to the CPU using the to() method.
    The function iterates over the list of word pairs and generates embeddings for each word in the pair using the pre-trained tokenizer and model. The inputs are tokenized and converted to PyTorch tensors, and then passed through the model to generate embeddings. The embeddings generated for each word in the pair are averaged to obtain the plain embeddings of the word pair, which are then appended to the "embeddings" list.
    The function then computes the cosine similarity between the plain embeddings of each word pair using the cosine_similarity function from the sklearn library. The cosine similarity score is computed using the formula (a * b) / (||a|| * ||b||), where a and b are the plain embeddings of two words.
    Finally, the cosine similarity scores are added to the input dataframe "term_matching" as a new column, and the updated dataframe is returned.
    Overall, this function is used to obtain the cosine similarity scores between the plain embeddings of a list of word pairs generated by a pre-trained transformer model. The plain embeddings are obtained by averaging the embeddings of each word in the pair, and the cosine similarity is computed using the cosine_similarity function.


3. def preprocess(text):
    The function takes a single input parameter "text", which is a raw text input.
    The raw text is passed through the SpaCy NLP pipeline using the nlp() function to obtain a processed Doc object named "doc_raw".
    The function initializes an empty list called "ner_raw" to store the Named Entity Recognition (NER) labels of the entities identified in the input text.
    The function iterates over the entities identified in the "doc_raw" object using the ents attribute and extracts their lemma and NER label. The extracted information is stored in a list called "ner_temp", which is then appended to the "ner_raw" list.
    The function replaces the text "y/o" (short for "years old") with the string "year old" using the replace() method.
    The processed text is passed through the SpaCy NLP pipeline again to obtain a new "doc" object.
    The function creates a new list called "ner_preprocess" to store the NER labels of the entities identified in the preprocessed text.
    The function extracts the lemma and NER label of each entity identified in the preprocessed text using the same approach as step 4, and appends the information to the "ner_preprocess" list.
    The function then processes the text further by iterating over each token in the "doc" object using a list comprehension. The token is included in the final processed text if it meets the following conditions: (a) it is not a stop word, (b) it is not a punctuation mark, (c) it is not a whitespace character, and (d) its lemma is not an empty string. The extracted lemmas are converted to lowercase, stripped of any trailing spaces, and commas are removed. The resulting list of lemmas is then joined together to form a single string.
    The function returns a Pandas Series object containing three items: (a) the list of NER labels extracted from the raw text ("ner_raw"), (b) the preprocessed text, and (c) the list of NER labels extracted from the preprocessed text ("ner_preprocess").
    Overall, this function is used to preprocess raw text data by extracting NER labels, replacing certain abbreviations with full phrases, removing stop words and punctuation marks, and converting words to their lemmas. The preprocessed text and the NER labels for the raw and preprocessed text are returned as a Pandas Series object.


4. def preprocess_notes(medical_notes,label_dict):
    This function takes in two parameters, medical_notes and label_dict. medical_notes is a Pandas DataFrame containing the medical notes data, and label_dict is a dictionary that maps category labels to numerical codes.
    The function first prints the value counts of the category column of medical_notes, which gives an idea of the distribution of the data.
    Next, it creates a new column called category_label in medical_notes by applying a lambda function to the category column. The lambda function uses label_dict to map each category label to its corresponding numerical code.
    The function then applies the preprocess function to the notes column of medical_notes using the Pandas apply method. The preprocess function processes each medical note and tries to capture Named Entity Recognition (NER) tags. The output of the preprocess function is a Pandas Series containing three columns: ner_notes, notes_preprocess, and ner_preprocess_notes.
    Finally, the function returns the modified medical_notes DataFrame and the label_dict dictionary.

5. def Analyze_models(model_dict, word_pairs, medical_notes, term_matching,label_dict):
    This is a function that fine-tunes different pre-trained models on medical notes data, evaluates their performance, and checks the similarity between pairs of words using the models' embeddings.
    Arguments:
        model_dict (dictionary): A dictionary containing the names and pretrained models to use.
        word_pairs (list): A list of tuples of word pairs to compare for similarity.
        medical_notes (pandas DataFrame): A pandas DataFrame containing medical notes data.
        term_matching (dictionary): A dictionary of terms and their embeddings to match for similarity.
        label_dict (dict) : A dictionary of category and their numerical labels.

    Returns:
        A tuple containing the medical notes DataFrame, the updated term_matching dictionary, and a dictionary of results for each model.
        The function first loads different models and tokenizers using the model_dict dictionary, and then it gets the direct embeddings from the model and checks the similarity between pairs in term_matching using the get_plain_embedding_similarity function. The function then fine-tunes each model on both raw and preprocessed medical notes data using the fine_tune function, and stores the performance metrics and classification reports in a dictionary called AllResult. Finally, the function returns a tuple containing the original medical notes DataFrame, the updated term_matching dictionary, and the AllResult dictionary.


6. def fine_tune(mod, model, tokenizer, medical_notes, label_dict,type_data,term_matching,word_pairs):
    This function fine-tunes a given transformer model on a medical notes dataset, and returns performance metrics,term matching embedding similarity and saves the fine-tuned model, tokenizer and configuration, training and test files.
    Arguments:
        mod (string): Name of the transformer model to be fine-tuned.
        model (transformer model): Pre-trained transformer model to be fine-tuned.
        tokenizer (tokenizer): Pre-trained tokenizer corresponding to the model.
        medical_notes (pandas dataframe): Pandas dataframe containing the medical notes dataset.
        label_dict (dictionary): Dictionary mapping category labels to category names.
        type_data (string): Name of the column containing the text data in the medical_notes dataframe.
        term_matching (dictionary): Dictionary containing terms for term-matching analysis.
        word_pairs (list): List of tuples containing word pairs for term-matching analysis.

    Returns:
        result_scores (dictionary): Dictionary containing various performance metrics.
        class_report (string): String containing a classification report of the model.
        term_matching (dictionary): Dictionary containing term matching embeddings after fine-tuning.

7. def get_score(y_ground,y_pred_ground,average='weighted'):
    This function calculates and returns the Accuracy, Precision, Recall and F1-Score based on the ground truth labels and predicted labels.
    Function arguments:
        y_ground: A list or array of ground truth labels.
        y_pred_ground: A list or array of predicted labels.
        average: (Optional) The type of averaging to be used for Precision, Recall and F1-Score. The default is 'weighted'.
    
    Returns:
        A pandas dataframe containing the Accuracy, Precision, Recall and F1-Score.

8. def PerformanceMetrics(y_ground, y_pred_ground, label_dict):
    Calculates and prints the performance metrics, including accuracy, precision, recall, f1-score, confusion matrix, and classification report.
    Parameters:
        y_ground (array-like): The ground truth labels.
        y_pred_ground (array-like): The predicted labels.
        label_dict (dict): A dictionary containing the label names as keys and their corresponding integer values as values.
        output_path (str): Model Specific Path to save confusion matrix

    Returns:
        tuple: A tuple containing the result_scores and classification_report_df DataFrames.

9. def get_confusion(y_ground, y_pred_ground, label_dict, sample_weight=None, normalize=None, xlab="Predicted Label", ylab="Actual Label"):
    Plot and saves the confusion matrix for the given ground truth and predicted labels.
    Args:
        - y_ground: array-like of shape (n_samples,), true labels for samples.
        - y_pred_ground: array-like of shape (n_samples,), predicted labels for samples.
        - label_dict: dict, a dictionary of label names and their corresponding indices in the confusion matrix.
        - sample_weight: array-like of shape (n_samples,), optional, Sample weights.
        - normalize: str or None, optional, 'true', 'pred', 'all', Normalize confusion matrix over the true (rows), predicted (columns) or all population. If None, confusion matrix will not be normalized.
        - xlab: str, optional, the label for the x-axis of the confusion matrix plot.
        - ylab: str, optional, the label for the y-axis of the confusion matrix plot.
        - output_path (str): Model Specific Path to save confusion matrix

    Returns:
    - None

10. def lime_prediction(model_name, text, save_path,label_dict_small, output_label = tuple(label_dict_small.values()), num_features=10, num_samples = 20)
    Generate an explanation for a given text using LIME.
    Args:
        model_name (str): The name of the model to use for prediction.
        text (str): The input text to generate an explanation for.
        save_path (str): The path to save the LIME explanation to.
        label_dict_small (dict): A dictionary mapping label names to integer ids.
        output_label (tuple, optional): A tuple of label ids to generate explanations for. Defaults to all labels.
        num_features (int, optional): The number of features to include in the explanation. Defaults to 10.
        num_samples (int, optional): The number of samples to generate for each explanation. Defaults to 20.

    Returns:
        None
